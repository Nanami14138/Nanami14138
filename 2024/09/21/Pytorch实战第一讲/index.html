<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="预习作业:掌握Numpy中有关张量维度的知识1.常用的机器学习，深度学习开源框架价绍PyTorch FaceBook2017年发布的全新机器学习包 功能很强大（强推） 擅长数据分析 支持动态图创建 支持GPU的Tensor库训练，加速计算 可以替代Numpy 简单实用 社区完善  TensorFlow 谷歌第二代机器学习系统 顾名思义，张量and流 由于架构很灵活，可移植性很好，工业领域用的很多，">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch实战第一讲">
<meta property="og:url" content="https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/index.html">
<meta property="og:site_name" content="Susprin&#39;s Blog">
<meta property="og:description" content="预习作业:掌握Numpy中有关张量维度的知识1.常用的机器学习，深度学习开源框架价绍PyTorch FaceBook2017年发布的全新机器学习包 功能很强大（强推） 擅长数据分析 支持动态图创建 支持GPU的Tensor库训练，加速计算 可以替代Numpy 简单实用 社区完善  TensorFlow 谷歌第二代机器学习系统 顾名思义，张量and流 由于架构很灵活，可移植性很好，工业领域用的很多，">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-09-21T07:39:23.469Z">
<meta property="article:modified_time" content="2024-09-21T07:59:47.483Z">
<meta property="article:author" content="Susprin14138">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/Nanami14138/images/vt_48x48.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/Nanami14138/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/Nanami14138/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Pytorch实战第一讲</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/Nanami14138/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/Nanami14138/">Home</a></li><!--
     --><!--
       --><li><a href="/Nanami14138/about/">About</a></li><!--
     --><!--
       --><li><a href="/Nanami14138/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/Nanami14138/">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/Nanami14138/2024/09/27/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/Nanami14138/2024/09/21/hello-world/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&text=Pytorch实战第一讲"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&is_video=false&description=Pytorch实战第一讲"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Pytorch实战第一讲&body=Check out this article: https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&name=Pytorch实战第一讲&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&t=Pytorch实战第一讲"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E4%B9%A0%E4%BD%9C%E4%B8%9A-%E6%8E%8C%E6%8F%A1Numpy%E4%B8%AD%E6%9C%89%E5%85%B3%E5%BC%A0%E9%87%8F%E7%BB%B4%E5%BA%A6%E7%9A%84%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">预习作业:掌握Numpy中有关张量维度的知识</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%B8%B8%E7%94%A8%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6%E4%BB%B7%E7%BB%8D"><span class="toc-number"></span> <span class="toc-text">1.常用的机器学习，深度学习开源框架价绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch"><span class="toc-number"></span> <span class="toc-text">PyTorch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow"><span class="toc-number"></span> <span class="toc-text">TensorFlow</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MindSpore"><span class="toc-number"></span> <span class="toc-text">MindSpore</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scikit-Learn"><span class="toc-number"></span> <span class="toc-text">Scikit-Learn</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-PyTorch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%91%88%E7%8E%B0%E6%96%B9%E5%BC%8F%EF%BC%9AScalar-Vector-Matrix-Tensor"><span class="toc-number"></span> <span class="toc-text">2.PyTorch中的数据呈现方式：Scalar,Vector,Matrix,Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%9CTorch-loves-tensor%E2%80%9D"><span class="toc-number"></span> <span class="toc-text">“Torch loves tensor”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%98%AFtensor-%EF%BC%8C%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%BA%E5%BC%A0%E9%87%8F"><span class="toc-number"></span> <span class="toc-text">默认的数据结构是tensor()，中文翻译为张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A5%E4%B8%8B%E5%A4%A7%E6%A0%87%E9%A2%98%E6%89%80%E8%AF%B4%E7%9A%84tensor%E9%83%BD%E6%8C%87PyTorch%E9%BB%98%E8%AE%A4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E5%85%B7%E4%BD%93%E6%9F%90%E4%B8%80%E4%B8%AA%E4%B8%89%E7%BB%B4%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-number"></span> <span class="toc-text">以下大标题所说的tensor都指PyTorch默认的数据结构，而不是具体某一个三维的张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%80%E4%BA%9Btensor"><span class="toc-number"></span> <span class="toc-text">2.1自定义一些tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%80%E4%BA%9B%E7%89%B9%E6%AE%8A%E7%9A%84tensor"><span class="toc-number"></span> <span class="toc-text">2.2初始化一些特殊的tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3Tensor%E5%9F%BA%E6%9C%AC%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number"></span> <span class="toc-text">2.3Tensor基本数学运算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4Tensor%E4%B8%8B%E6%A0%87%E8%AE%A1%E7%AE%97"><span class="toc-number"></span> <span class="toc-text">2.4Tensor下标计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5%E9%87%8D%E7%82%B9%EF%BC%9A%E5%AE%9E%E7%8E%B0PyTorch%E4%B8%AD%E7%9A%84Tensor%E4%B8%8ENumpy%E4%B8%AD%E7%9A%84Array%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-number"></span> <span class="toc-text">2.5重点：实现PyTorch中的Tensor与Numpy中的Array相互转换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%EF%BC%88%E6%8C%96%E5%9D%91%EF%BC%89CUDA%E5%AE%89%E8%A3%85"><span class="toc-number"></span> <span class="toc-text">3.（挖坑）CUDA安装</span></a>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Pytorch实战第一讲
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Susprin14138</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-09-21T07:39:23.469Z" class="dt-published" itemprop="datePublished">2024-09-21</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h4 id="预习作业-掌握Numpy中有关张量维度的知识"><a href="#预习作业-掌握Numpy中有关张量维度的知识" class="headerlink" title="预习作业:掌握Numpy中有关张量维度的知识"></a><em>预习作业:掌握Numpy中有关张量维度的知识</em></h4><h1 id="1-常用的机器学习，深度学习开源框架价绍"><a href="#1-常用的机器学习，深度学习开源框架价绍" class="headerlink" title="1.常用的机器学习，深度学习开源框架价绍"></a>1.常用的机器学习，深度学习开源框架价绍</h1><h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><ul>
<li>FaceBook2017年发布的全新机器学习包</li>
<li>功能很强大（强推）</li>
<li>擅长数据分析</li>
<li>支持动态图创建</li>
<li><strong>支持GPU的Tensor库训练，加速计算</strong></li>
<li><strong>可以替代Numpy</strong></li>
<li>简单实用</li>
<li><strong>社区完善</strong></li>
</ul>
<h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><ul>
<li>谷歌第二代机器学习系统</li>
<li>顾名思义，<strong>张量and流</strong></li>
<li>由于架构很灵活，可移植性很好，工业领域用的很多，真神</li>
<li>但不是很容易上手，所以看起来有点史</li>
<li>学术界不是很经常用</li>
</ul>
<h2 id="MindSpore"><a href="#MindSpore" class="headerlink" title="MindSpore"></a>MindSpore</h2><ul>
<li>史中史</li>
<li>唯一真史</li>
</ul>
<h2 id="Scikit-Learn"><a href="#Scikit-Learn" class="headerlink" title="Scikit-Learn"></a>Scikit-Learn</h2><ul>
<li>基于Python的机器学习模块</li>
<li>由David Cournapeau于2007年发起</li>
<li>基本功能分为6个模块：分类，回归，聚类，数据降维，模型选择，数据准备，预处理</li>
<li>对于具体的机器学习问题分为3个模块：数据准备&amp;预处理，模型选择&amp;训练，模型验证&amp;参数调优</li>
<li>支持多种格式的数据</li>
<li>用的也比较多</li>
<li>可以一试</li>
</ul>
<h1 id="2-PyTorch中的数据呈现方式：Scalar-Vector-Matrix-Tensor"><a href="#2-PyTorch中的数据呈现方式：Scalar-Vector-Matrix-Tensor" class="headerlink" title="2.PyTorch中的数据呈现方式：Scalar,Vector,Matrix,Tensor"></a>2.PyTorch中的数据呈现方式：Scalar,Vector,Matrix,Tensor</h1><h3 id="“Torch-loves-tensor”"><a href="#“Torch-loves-tensor”" class="headerlink" title="“Torch loves tensor”"></a><em>“Torch loves tensor”</em></h3><h3 id="默认的数据结构是tensor-，中文翻译为张量"><a href="#默认的数据结构是tensor-，中文翻译为张量" class="headerlink" title="默认的数据结构是tensor()，中文翻译为张量"></a>默认的数据结构是tensor()，中文翻译为张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直观理解：</span></span><br><span class="line">scalar<span class="comment">#点</span></span><br><span class="line">vector<span class="comment">#线</span></span><br><span class="line">matrix<span class="comment">#面</span></span><br><span class="line">tensor<span class="comment">#体</span></span><br></pre></td></tr></table></figure>

<h2 id="以下大标题所说的tensor都指PyTorch默认的数据结构，而不是具体某一个三维的张量"><a href="#以下大标题所说的tensor都指PyTorch默认的数据结构，而不是具体某一个三维的张量" class="headerlink" title="以下大标题所说的tensor都指PyTorch默认的数据结构，而不是具体某一个三维的张量"></a><em>以下大标题所说的tensor都指PyTorch默认的数据结构，而不是具体某一个三维的张量</em></h2><p>辛苦大家，适应一下这种说法</p>
<h2 id="2-1自定义一些tensor"><a href="#2-1自定义一些tensor" class="headerlink" title="2.1自定义一些tensor"></a>2.1自定义一些tensor</h2><p><strong>torch.tensor(xxxx):Create a tensor</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">scalar=torch.tensor(<span class="number">7</span>)</span><br><span class="line">vector=torch.tensor([<span class="number">7</span>,<span class="number">4</span>])</span><br><span class="line">MATRIX=torch.tensor([[<span class="number">0</span>,<span class="number">7</span>],[<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line">TENSOR=torch.tensor([[[<span class="number">0</span>,<span class="number">7</span>],[<span class="number">2</span>,<span class="number">1</span>]]])</span><br><span class="line"></span><br><span class="line">scalar</span><br><span class="line">vector</span><br><span class="line">MATRIX</span><br><span class="line">TENSOR</span><br></pre></td></tr></table></figure>
<p>tensor(7)<br>torch.tensor([7,4])<br>tensor([ [0, 7],<br>    [2, 1] ])<br>tensor([ [ [0, 7],<br>    [2, 1] ] ])</p>
<p><strong>ndim:Check the dimensions of scalar</strong><br><strong>维度计算快捷方法</strong>：数一数<strong>几层中括号</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scalar.ndim</span><br><span class="line">vector.ndim</span><br><span class="line">MATRIX.ndim</span><br><span class="line">TENSOR.nidm</span><br></pre></td></tr></table></figure>
<p>0<br>1<br>2<br>3</p>
<p><strong>item:Turn tensor into a Python number</strong><br>顾名思义，把物体（数字）从tensor中取出来看看他是个什么东西）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scalar.item</span><br></pre></td></tr></table></figure>
<p>7</p>
<p><strong>Check the shape of the vector,2 elements are put into the brackets</strong><br>涉及到一些维度的知识</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector.shape</span><br><span class="line">MATRIX.shape</span><br><span class="line">TENSOR.shape</span><br></pre></td></tr></table></figure>
<p>torch.Size([2])<br>torch.Size([2, 2])<br>torch.Size([1, 2, 2])</p>
<h2 id="2-2初始化一些特殊的tensor"><a href="#2-2初始化一些特殊的tensor" class="headerlink" title="2.2初始化一些特殊的tensor"></a>2.2初始化一些特殊的tensor</h2><p>Tensor默认数据类型为float.32</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Tensor datatypes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Deafault datatype for tensors is float32</span></span><br><span class="line"></span><br><span class="line">float_32_tensor=torch.tensor([<span class="number">3.0</span>,<span class="number">6.0</span>,<span class="number">9.0</span>],</span><br><span class="line"></span><br><span class="line">                             dtype=<span class="literal">None</span>,<span class="comment"># defaults to None, which is torch.float32 or whatever datatype is passed</span></span><br><span class="line"></span><br><span class="line">                             device=<span class="literal">None</span>,<span class="comment"># defaults to None, which uses the default tensor type</span></span><br><span class="line"></span><br><span class="line">                             requires_grad=<span class="literal">False</span>)<span class="comment"># if True, operations performed on the tensor are recorded</span></span><br><span class="line"></span><br><span class="line">float_32_tensor.shape,float_32_tensor.dtype</span><br></pre></td></tr></table></figure>
<p>(torch.Size([3]), torch.float32)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">float_16_tensor=torch.tensor([<span class="number">3.0</span>,<span class="number">6.0</span>,<span class="number">9.0</span>],</span><br><span class="line">							 dtype=torch.float16)</span><br><span class="line">float_16_tensor.dtype</span><br></pre></td></tr></table></figure>
<p>torch.float16</p>
<p>随机初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">random_tensor=torch.rand(size=(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">random_tensor,random_tensor.dtype</span><br></pre></td></tr></table></figure>
<p>(tensor([ [ 0.1093, 0.8989, 0.8195, 0.3373] , [ 0.1179, 0.2801, 0.3570, 0.5607], [0.8991, 0.3874, 0.1780, 0.7507] ] ), torch.float32)</p>
<p>全0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zeros=torch.zeros(size=(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">zeros,zeros.dtype</span><br></pre></td></tr></table></figure>


<p>全1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ones=torch.ones(size=(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">ones,ones.dtype</span><br></pre></td></tr></table></figure>

<p>等差数列<del>笑声</del></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#In Python, you can use range() to create a range. However in PyTorch, torch.range() is deprecated and may show an error in the future.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#You can use torch.arange(start, end, step) to do so</span></span><br><span class="line"></span><br><span class="line">zero_to_ten=torch.arange(start=<span class="number">0</span>,end=<span class="number">10</span>,step=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">zero_to_ten</span><br></pre></td></tr></table></figure>
<h2 id="2-3Tensor基本数学运算"><a href="#2-3Tensor基本数学运算" class="headerlink" title="2.3Tensor基本数学运算"></a>2.3Tensor基本数学运算</h2><p>对于每个元素：<br>加<br>减<br>乘</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor + <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>tensor([11, 12, 13])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor * <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>tensor([10, 20, 30])</p>
<p>注意：tensor自身没变</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor</span><br></pre></td></tr></table></figure>
<p>tensor([1, 2, 3])</p>
<p>reassign:tensor自身改变</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add and reassign</span></span><br><span class="line">tensor = tensor - <span class="number">10</span></span><br><span class="line">tensor</span><br></pre></td></tr></table></figure>
<p>tensor([-9, -8, -7])</p>
<p>使用multiply函数对tensor中每个元素乘10<br>但是tensor自身不变</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.multiply(tensor,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>tensor([10, 20, 30])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Original tensor is still unchanged</span></span><br><span class="line">tensor</span><br></pre></td></tr></table></figure>

<p>逐个元素相乘</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tensor,<span class="string">&quot;*&quot;</span>,tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Equals=&quot;</span>,tensor*tensor)</span><br></pre></td></tr></table></figure>
<p>tensor([1, 2, 3]) * tensor([1, 2, 3])<br>Equals&#x3D; tensor([1, 4, 9])</p>
<p>设置随机数tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">random_tensor_A=torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">random_tensor_B=torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does tensor_A equal to tensor_B?\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">random_tensor_A==random_tensor_B</span><br></pre></td></tr></table></figure>
<p>Does tensor_A equal to tensor_B?<br>tensor([[False, False, False, False], [False, False, False, False], [False, False, False, False]])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">RANDOM_SEED=<span class="number">42</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(seed=RANDOM_SEED)</span><br><span class="line"></span><br><span class="line">random_tensor_c=torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">torch.manual_seed(seed=RANDOM_SEED)</span><br><span class="line"></span><br><span class="line">random_tensor_d=torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">random_tensor_d==random_tensor_c</span><br></pre></td></tr></table></figure>
<p>tensor( [ [True, True, True, True],[ True, True, True, True ], [True, True, True, True ] ] )</p>
<p><strong>重点：矩阵乘法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Matrix Multiplication</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#PyTorch implements matrix multiplication functionality in the torch.matmul() method</span></span><br><span class="line"></span><br><span class="line">torch.matmul(tensor,tensor)</span><br></pre></td></tr></table></figure>

<p>一个很容易犯的错<br><strong>这里需要对矩阵b进行转置处理</strong><br><strong>否则无法按照定义进行矩阵乘法计算</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tensor_A=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line"></span><br><span class="line">                      [<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line"></span><br><span class="line">                      [<span class="number">5</span>,<span class="number">6</span>]],dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">tensor_B=torch.tensor([[<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line"></span><br><span class="line">                      [<span class="number">9</span>,<span class="number">10</span>],</span><br><span class="line"></span><br><span class="line">                      [<span class="number">11</span>,<span class="number">12</span>]],dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.matmul(tensor_A,tensor_B)-------Error</span></span><br><span class="line"></span><br><span class="line">torch.matmul(tensor_A,tensor_B.T)</span><br></pre></td></tr></table></figure>

<p>定义一个[2,6]由随机数组成的线性层<br>利用torch.nn.Linear，实现矩阵相乘</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置随机数</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">linear=torch.nn.Linear(in_features=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">                      out_features=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">x=tensor_A</span><br><span class="line"></span><br><span class="line">output=linear(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
<p>torch.Size([3, 2])<br>torch.Size([3, 6])</p>
<h2 id="2-4Tensor下标计算"><a href="#2-4Tensor下标计算" class="headerlink" title="2.4Tensor下标计算"></a>2.4Tensor下标计算</h2><p>一级索引<br>二级索引<br>三级索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s index bracket by bracket</span></span><br><span class="line">a=torch.arrange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>).reshape(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#(tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]), torch.Size([1, 3, 3]))</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First square bracket:\n<span class="subst">&#123;a[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Second square bracket: <span class="subst">&#123;a[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Third square bracket: <span class="subst">&#123;a[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>First square bracket: tensor([ [ 1, 2, 3], [4, 5, 6], [7, 8, 9] ] )<br>Second square bracket: tensor([1, 2, 3])<br>Third square bracket: 1</p>
<p>考考聪明的你们<del>我是笨蛋</del></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a[:,<span class="number">0</span>]</span><br><span class="line">a[:,<span class="number">1</span>]</span><br><span class="line">a[<span class="number">0</span>,<span class="number">0</span>,:]</span><br><span class="line">a[<span class="number">0</span>,<span class="number">1</span>,:]</span><br><span class="line">a[:,:,<span class="number">0</span>]</span><br><span class="line">a[:,:,<span class="number">1</span>]</span><br><span class="line">a[:,:,<span class="number">2</span>]</span><br><span class="line">a[:,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">a[:,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">a[:,<span class="number">2</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>


<h2 id="2-5重点：实现PyTorch中的Tensor与Numpy中的Array相互转换"><a href="#2-5重点：实现PyTorch中的Tensor与Numpy中的Array相互转换" class="headerlink" title="2.5重点：实现PyTorch中的Tensor与Numpy中的Array相互转换"></a>2.5重点：实现PyTorch中的Tensor与Numpy中的Array相互转换</h2><p>torch.from_numpy(array):Numpy array-&gt;Pytorch Tensor</p>
<p>torch.tensor.numpy():Pytorch Tensor-&gt;Numpy array</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">array=np.arange(<span class="number">1.0</span>,<span class="number">8.0</span>)</span><br><span class="line"></span><br><span class="line">tensor=torch.from_numpy(array).<span class="built_in">type</span>(torch.float32)</span><br><span class="line"></span><br><span class="line">array,tensor</span><br></pre></td></tr></table></figure>
<p>(array([1., 2., 3., 4., 5., 6., 7.]), tensor([1., 2., 3., 4., 5., 6., 7.]))</p>
<p>array改变，tensor不变</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array=array+<span class="number">1</span></span><br><span class="line">array,tensor</span><br></pre></td></tr></table></figure>
<p>(array([2., 3., 4., 5., 6., 7., 8.]), tensor([1., 2., 3., 4., 5., 6., 7.]))</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor=torch.ones(<span class="number">7</span>)<span class="comment">#dtype=float32</span></span><br><span class="line">numpy_tensor=tensor.numpy()<span class="comment"># float32 unless changed</span></span><br><span class="line">tensor,numpy_tensor</span><br></pre></td></tr></table></figure>
<p>(tensor([1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1.], dtype&#x3D;float32))</p>
<h1 id="3-（挖坑）CUDA安装"><a href="#3-（挖坑）CUDA安装" class="headerlink" title="3.（挖坑）CUDA安装"></a>3.（挖坑）CUDA安装</h1><p>检查CUDA是否安装成功</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>
<p>True</p>
<p>True为安装成功</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/Nanami14138/">Home</a></li>
        
          <li><a href="/Nanami14138/about/">About</a></li>
        
          <li><a href="/Nanami14138/archives/">Writing</a></li>
        
          <li><a href="/Nanami14138/">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E4%B9%A0%E4%BD%9C%E4%B8%9A-%E6%8E%8C%E6%8F%A1Numpy%E4%B8%AD%E6%9C%89%E5%85%B3%E5%BC%A0%E9%87%8F%E7%BB%B4%E5%BA%A6%E7%9A%84%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">预习作业:掌握Numpy中有关张量维度的知识</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%B8%B8%E7%94%A8%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6%E4%BB%B7%E7%BB%8D"><span class="toc-number"></span> <span class="toc-text">1.常用的机器学习，深度学习开源框架价绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch"><span class="toc-number"></span> <span class="toc-text">PyTorch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow"><span class="toc-number"></span> <span class="toc-text">TensorFlow</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MindSpore"><span class="toc-number"></span> <span class="toc-text">MindSpore</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scikit-Learn"><span class="toc-number"></span> <span class="toc-text">Scikit-Learn</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-PyTorch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%91%88%E7%8E%B0%E6%96%B9%E5%BC%8F%EF%BC%9AScalar-Vector-Matrix-Tensor"><span class="toc-number"></span> <span class="toc-text">2.PyTorch中的数据呈现方式：Scalar,Vector,Matrix,Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%9CTorch-loves-tensor%E2%80%9D"><span class="toc-number"></span> <span class="toc-text">“Torch loves tensor”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%98%AFtensor-%EF%BC%8C%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%BA%E5%BC%A0%E9%87%8F"><span class="toc-number"></span> <span class="toc-text">默认的数据结构是tensor()，中文翻译为张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A5%E4%B8%8B%E5%A4%A7%E6%A0%87%E9%A2%98%E6%89%80%E8%AF%B4%E7%9A%84tensor%E9%83%BD%E6%8C%87PyTorch%E9%BB%98%E8%AE%A4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E5%85%B7%E4%BD%93%E6%9F%90%E4%B8%80%E4%B8%AA%E4%B8%89%E7%BB%B4%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-number"></span> <span class="toc-text">以下大标题所说的tensor都指PyTorch默认的数据结构，而不是具体某一个三维的张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%80%E4%BA%9Btensor"><span class="toc-number"></span> <span class="toc-text">2.1自定义一些tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%80%E4%BA%9B%E7%89%B9%E6%AE%8A%E7%9A%84tensor"><span class="toc-number"></span> <span class="toc-text">2.2初始化一些特殊的tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3Tensor%E5%9F%BA%E6%9C%AC%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number"></span> <span class="toc-text">2.3Tensor基本数学运算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4Tensor%E4%B8%8B%E6%A0%87%E8%AE%A1%E7%AE%97"><span class="toc-number"></span> <span class="toc-text">2.4Tensor下标计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5%E9%87%8D%E7%82%B9%EF%BC%9A%E5%AE%9E%E7%8E%B0PyTorch%E4%B8%AD%E7%9A%84Tensor%E4%B8%8ENumpy%E4%B8%AD%E7%9A%84Array%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-number"></span> <span class="toc-text">2.5重点：实现PyTorch中的Tensor与Numpy中的Array相互转换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%EF%BC%88%E6%8C%96%E5%9D%91%EF%BC%89CUDA%E5%AE%89%E8%A3%85"><span class="toc-number"></span> <span class="toc-text">3.（挖坑）CUDA安装</span></a>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&text=Pytorch实战第一讲"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&is_video=false&description=Pytorch实战第一讲"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Pytorch实战第一讲&body=Check out this article: https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&title=Pytorch实战第一讲"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&name=Pytorch实战第一讲&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://nanami14138.github.io/Nanami14138/2024/09/21/Pytorch%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%80%E8%AE%B2/&t=Pytorch实战第一讲"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    Susprin14138
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/Nanami14138/">Home</a></li><!--
     --><!--
       --><li><a href="/Nanami14138/about/">About</a></li><!--
     --><!--
       --><li><a href="/Nanami14138/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/Nanami14138/">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/Nanami14138/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
